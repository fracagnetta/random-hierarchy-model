{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DATASET INITIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import datasets\n",
    "import importlib\n",
    "importlib.reload(datasets)\n",
    "\n",
    "from datasets.utils import dec2bin, dec2base\n",
    "from datasets.random_hierarchy_model import sample_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "v = 32\n",
    "m = 2\n",
    "\n",
    "L = 2\n",
    "s = 2\n",
    "\n",
    "input_size = s**L # number of pixels, actual input size is (input_size x num_features) because of one-hot encoding\n",
    "num_data = n * (m**((s**L-1)//(s-1))) # total number of data\n",
    "print(input_size, num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARD SAMPLING (WITHOUT REPLACEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling seed: 26283259\n",
      "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_is_protocol', 'features', 'labels', 'num_classes', 'num_features', 'num_layers', 'num_synonyms', 'rules', 'transform', 'tuple_size']\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "random.seed()\n",
    "seed_rules = 12345678 # seed of the random hierarchy model\n",
    "seed_sample = random.randrange(10000000,99999999)\n",
    "print('sampling seed:', seed_sample)\n",
    "\n",
    "train_size = -1 # size of the training set\n",
    "test_size = 0 # size of the test set\n",
    "input_format = 'long' # alternative: onehot\n",
    "# to generate the full dataset: set trainset=num_data, test_size=0\n",
    "bonus = dict.fromkeys(['tree', 'noise', 'synonyms', 'size'])\n",
    "bonus['size'] = 4\n",
    "\n",
    "dataset = datasets.RandomHierarchyModel(\n",
    "    num_features=v, # vocabulary size\n",
    "    num_synonyms=m, # features multiplicity\n",
    "    num_layers=L, # number of layers\n",
    "    num_classes=n, # number of classes\n",
    "    tuple_size=s, # number of branches of the tree\n",
    "    seed_rules=seed_rules,\n",
    "    seed_sample=seed_sample,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    input_format=input_format,\n",
    "    whitening=0, # 1 to whiten the input\n",
    "    replacement=False,\n",
    "    bonus=bonus\n",
    ")\n",
    "\n",
    "print(dir(dataset)) \n",
    "# for the input points call trainset.input\n",
    "print(dataset.features.size()) # dimension: train_size x num_features x input_size\n",
    "# for the labels call trainset.output\n",
    "print(dataset.labels.size()) # dimension: train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15,  5,  9,  2],\n",
      "        [15,  5, 16, 19],\n",
      "        [ 9,  6,  9,  2],\n",
      "        [ 9,  6, 16, 19],\n",
      "        [32, 12, 18, 31],\n",
      "        [32, 12, 14, 25],\n",
      "        [12,  8, 18, 31],\n",
      "        [12,  8, 14, 25],\n",
      "        [15,  8,  3, 27],\n",
      "        [15,  8, 30,  9],\n",
      "        [10, 16,  3, 27],\n",
      "        [10, 16, 30,  9],\n",
      "        [15, 13,  7, 24],\n",
      "        [15, 13, 23, 20],\n",
      "        [31, 29,  7, 24],\n",
      "        [31, 29, 23, 20]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([1, 1, 1, 1])\n",
      "1 tensor([[ 4, 30],\n",
      "        [ 4, 30],\n",
      "        [ 4, 30],\n",
      "        [ 4, 30]])\n"
     ]
    }
   ],
   "source": [
    "for k in bonus['tree'].keys():\n",
    "    print(k,bonus['tree'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bonus['synonyms'].keys():\n",
    "    print(k,bonus['synonyms'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bonus['noise'].keys():\n",
    "    print(k,bonus['noise'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.features\n",
    "print(x.size())\n",
    "\n",
    "if 'onehot' in input_format:\n",
    "    print(x.mean(dim=1).mean())\n",
    "    print(x.norm(dim=1).mean())\n",
    "\n",
    "elif 'long' in input_format:\n",
    "    for i in range(x.size(0)):\n",
    "        print(x[i,:], dataset.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(dataset.rules)\n",
    "print('rules: list of length ', len(dataset.rules), ',')\n",
    "print('first element of size ', dataset.rules[0].size(), ', (num_classes x num_synonyms x tuple_size)')\n",
    "\n",
    "for l in range(1,L):\n",
    "    print(f'{l+1}-th element of size ', dataset.rules[l].size(), ', (num_features x num_synonyms x tuple_size)')\n",
    "print('rules[l][v,j] = j-th rep of the v-th level-(L-l) feature,')\n",
    "print('e.g. list of tuples corresponding to class 0:')\n",
    "print(dataset.rules[0][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMPLING WITH REPLACEMENT (REQUIRED FOR DATASET LARGER THAN sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "seed_rules = 12345678 # seed of the random hierarchy model\n",
    "seed_sample = random.randrange(10000000,99999999)\n",
    "print('sampling seed:', seed_sample)\n",
    "\n",
    "train_size = -1 # size of the training set\n",
    "test_size = 0 # size of the test set\n",
    "input_format = 'long' # alternative: onehot\n",
    "# to generate the full dataset: set trainset=num_data, test_size=0\n",
    "\n",
    "dataset = datasets.RandomHierarchyModel(\n",
    "    num_features=v, # vocabulary size\n",
    "    num_synonyms=m, # features multiplicity\n",
    "    num_layers=L, # number of layers\n",
    "    num_classes=n, # number of classes\n",
    "    tuple_size=s, # number of branches of the tree\n",
    "    seed_rules=seed_rules,\n",
    "    seed_sample=seed_sample,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    input_format=input_format,\n",
    "    whitening=0, # 1 to whiten the input\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.features\n",
    "print(x.size())\n",
    "\n",
    "if 'onehot' in input_format:\n",
    "    print(x.mean(dim=1).mean())\n",
    "    print(x.norm(dim=1).mean())\n",
    "\n",
    "elif 'long' in input_format:\n",
    "    for i in range(x.size(0)):\n",
    "        print(x[i,:], dataset.labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PERTURBATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_from_indices(samples, rules, v, n, m, s, L, bonus):\n",
    "    \"\"\"\n",
    "    Create data of the Random Hierarchy Model starting from a set of rules and the sampled indices.\n",
    "\n",
    "    Args:\n",
    "        samples: A tensor of size [batch_size, I], with I from 0 to max_data-1, containing the indices of the data to be sampled.\n",
    "        rules: A dictionary containing the rules for each level of the hierarchy.\n",
    "        n: The number of classes (int).\n",
    "        m: The number of synonymic lower-level representations (multiplicity, int).\n",
    "        s: The size of lower-level representations (int).\n",
    "        L: The number of levels in the hierarchy (int).\n",
    "        bonus: Dictionary for additional output (list), includes 'noise' (randomly replace one symbol at each level), . Includes 'size' for the number of additional data. TODO: add custom positions for 'noise'\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the inputs and outputs of the model (plus additional output in bonus dict).\n",
    "    \"\"\"\n",
    "    max_data = n * m ** ((s**L-1)//(s-1))\n",
    "    data_per_hl = max_data // n \t# div by num_classes to get number of data per class\n",
    "\n",
    "    high_level = samples.div(data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get class index (run in range(n))\n",
    "    low_level = samples % data_per_hl\t\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "    labels = high_level\t\t# labels are the classes (features of highest level)\n",
    "    features = labels\t\t# init input features as labels (rep. size 1)\n",
    "    size = 1\n",
    "\n",
    "    if bonus:\n",
    "        if 'size' not in bonus.keys():\n",
    "            bonus['size'] = samples.size(0)\n",
    "        if 'noise' in bonus:\t# add corrupted version of the last bonus[-1] data\n",
    "            noise = {}\n",
    "            noise[L] = torch.clone(features[-bonus['size']:])\t# copy current representation (labels)...\n",
    "            noise[L][:] = torch.randint(n, (bonus['size'],))\t# ...and randomly change it\n",
    "            bonus['noise'] = noise\n",
    "        if 'synonyms' in bonus:\n",
    "            synonyms = {}\n",
    "            bonus['synonyms'] = synonyms\n",
    "\n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        choices = m**(size)\n",
    "        data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level feature\n",
    "\n",
    "        high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "        high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "        if 'synonyms' in bonus:\n",
    "\n",
    "            for ell in synonyms.keys():\t# propagate modified data down the tree\n",
    "                synonyms[ell] = rules[l][synonyms[ell], high_level[-bonus['size']:]]\n",
    "                synonyms[ell] = synonyms[ell].flatten(start_dim=1)\n",
    "\n",
    "            high_level_syn = torch.clone(high_level[-bonus['size']:]) # copy current representation indices...\n",
    "            if l==0:\n",
    "                high_level_syn[:] = torch.randint(m, (high_level_syn.size(0),)) # ... and randomly change it (only one index at the highest level)\n",
    "            else:\n",
    "                high_level_syn[:,-2] = torch.randint(m, (high_level_syn.size(0),))# ... and randomly change the next-to-last\n",
    "            synonyms[L-l] = torch.clone(features[-bonus['size']:])\n",
    "            synonyms[L-l] = rules[l][synonyms[L-l], high_level_syn]\n",
    "            synonyms[L-l] = synonyms[L-l].flatten(start_dim=1)\n",
    "            #TODO: add custom positions for 'synonyms'\n",
    "        \n",
    "        features = rules[l][features, high_level]\t        \t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "        features = features.flatten(start_dim=1)\t\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "        size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "        low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "        if 'noise' in bonus:\n",
    "\n",
    "            for ell in noise.keys():\t# propagate modified data down the tree\n",
    "                noise[ell] = rules[l][noise[ell], high_level[-bonus['size']:]]\n",
    "                noise[ell] = noise[ell].flatten(start_dim=1)\n",
    "\n",
    "            noise[L-l-1] = torch.clone(features[-bonus['size']:])\t# copy current representation ...\n",
    "            noise[L-l-1][:,-2] = torch.randint(v, (bonus['size'],))\t# ... and randomly change the next-to-last feature\n",
    "            #TODO: add custom positions for 'noise'\n",
    "\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "v = 32\n",
    "m = 2\n",
    "\n",
    "L = 2\n",
    "s = 2\n",
    "\n",
    "input_size = s**L # number of pixels, actual input size is (input_size x num_features) because of one-hot encoding\n",
    "max_data = n * (m**((s**L-1)//(s-1))) # total number of data\n",
    "print(input_size, max_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLE NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_rules = 12345678 # seed of the random hierarchy model\n",
    "rules = sample_rules( v, n, m, s, L, seed=seed_rules)\n",
    "samples = torch.arange( max_data)\n",
    "\n",
    "bonus = dict.fromkeys(['noise', 'size'])\n",
    "bonus['size'] = 4\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = n * m ** ((s**L-1)//(s-1))\n",
    "data_per_hl = max_data // n \t# div by num_classes to get number of data per class\n",
    "\n",
    "high_level = samples.div(data_per_hl, rounding_mode='floor')\n",
    "low_level = samples % data_per_hl\t\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "labels = high_level\t\t# labels are the classes (features of highest level)\n",
    "features = labels\t\t# init input features as labels (rep. size 1)\n",
    "size = 1\n",
    "\n",
    "if 'noise' in bonus:\n",
    "    noise = {}\n",
    "    noise[L] = torch.clone(features[-bonus['size']:]) # copy current representation (labels)...\n",
    "    noise[L][:] = torch.randint(n, (bonus['size'],)) # ...and randomly change it\n",
    "    bonus['noise'] = noise\n",
    "\n",
    "print(features)\n",
    "print(noise[L])\n",
    "\n",
    "\n",
    "for l in range(L):\n",
    "\n",
    "    choices = m**size\n",
    "    data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level featur\n",
    "\n",
    "    high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "    high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "    features = rules[l][features, high_level]\t\t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "    features = features.flatten(start_dim=1)\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "    size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "    low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "    if 'noise' in bonus:\n",
    "        for ell in noise.keys(): # propagate modified data\n",
    "            noise[ell] = rules[l][noise[ell], high_level[-bonus['size']:]]\n",
    "            noise[ell] = noise[ell].flatten(start_dim=1)\n",
    "\n",
    "        noise[L-l-1] = torch.clone(features[-bonus['size']:]) # copy current representation ...\n",
    "        noise[L-l-1][:,-2] = torch.randint(v, (bonus['size'],)) # ... and randomly change the next-to-last feature\n",
    "\n",
    "    print(features)\n",
    "\n",
    "    for key in noise.keys():\n",
    "        print(noise[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bonus['noise'])\n",
    "print(bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testable by looking at the rule matrices rules[0] and rules[1] (recall rules[l][v,j] = j-th rep of the v-th level-(L-l) feature) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SAMPLE SYNONYMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_rules = 12345678 # seed of the random hierarchy model\n",
    "rules = sample_rules( v, n, m, s, L, seed=seed_rules)\n",
    "samples = torch.arange( max_data)\n",
    "\n",
    "bonus = dict.fromkeys(['synonyms', 'size'])\n",
    "bonus['size'] = 4\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = n * m ** ((s**L-1)//(s-1))\n",
    "data_per_hl = max_data // n \t# div by num_classes to get number of data per class\n",
    "\n",
    "high_level = samples.div(data_per_hl, rounding_mode='floor')\n",
    "low_level = samples % data_per_hl\t\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "labels = high_level\t\t# labels are the classes (features of highest level)\n",
    "features = labels\t\t# init input features as labels (rep. size 1)\n",
    "size = 1\n",
    "\n",
    "if 'synonyms' in bonus:\n",
    "    synonyms = {}\n",
    "    bonus['synonyms'] = synonyms\n",
    "\n",
    "print(features)\n",
    "\n",
    "for l in range(L):\n",
    "\n",
    "    choices = m**size\n",
    "    data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level featur\n",
    "\n",
    "    high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "    high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "    if 'synonyms' in bonus:\n",
    "\n",
    "        for ell in synonyms.keys(): # propagate modified data\n",
    "            synonyms[ell] = rules[l][synonyms[ell], high_level[-bonus['size']:]]\n",
    "            synonyms[ell] = synonyms[ell].flatten(start_dim=1)\n",
    "\n",
    "        high_level_syn = torch.clone(high_level[-bonus['size']:])\n",
    "        if l==0:\n",
    "            high_level_syn[:] = torch.randint(m, (high_level_syn.size(0),))\n",
    "        else:\n",
    "            high_level_syn[:,-2] = torch.randint(m, (high_level_syn.size(0),))\n",
    "        synonyms[L-l] = torch.clone(features[-bonus['size']:])\n",
    "        synonyms[L-l] = rules[l][synonyms[L-l], high_level_syn]\n",
    "        synonyms[L-l] = synonyms[L-l].flatten(start_dim=1)\n",
    "        print(high_level_syn)\n",
    "        for key in synonyms.keys():\n",
    "            print(synonyms[key])\n",
    "\n",
    "    features = rules[l][features, high_level]\t\t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "    features = features.flatten(start_dim=1)\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "    size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "    low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "    print(high_level)\n",
    "    print(features)\n",
    "\n",
    "# for l in range(L):\n",
    "\n",
    "#     choices = m**size\n",
    "#     data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level featur\n",
    "\n",
    "#     high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "#     high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "#     features = rules[l][features, high_level]\t\t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "#     features = features.flatten(start_dim=1)\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "#     size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "#     low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "#     if 'noise' in bonus:\n",
    "#         for ell in noise.keys(): # propagate modified data\n",
    "#             noise[ell] = rules[l][noise[ell], high_level[-bonus['size']:]]\n",
    "#             noise[ell] = noise[ell].flatten(start_dim=1)\n",
    "\n",
    "#         noise[L-l-1] = torch.clone(features[-bonus['size']:]) # copy current representation ...\n",
    "#         noise[L-l-1][:,-2] = torch.randint(v, (bonus['size'],)) # ... and randomly change the next-to-last feature\n",
    "\n",
    "#     print(features)\n",
    "\n",
    "#     for key in noise.keys():\n",
    "#         print(noise[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rhm",
   "language": "python",
   "name": "rhm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
