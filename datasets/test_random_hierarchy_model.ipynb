{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DATASET INITIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import datasets\n",
    "import importlib\n",
    "importlib.reload(datasets)\n",
    "\n",
    "from datasets.utils import dec2bin, dec2base, base2dec\n",
    "from datasets.random_hierarchy_model import sample_rules, sample_data_from_labels, sample_data_from_labels_unif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 32\n"
     ]
    }
   ],
   "source": [
    "v=4    # The number of values each variable can take (vocabulary size, int).\n",
    "n=v    # The number of classes (int).\n",
    "m=2     # The number of synonymic lower-level representations (multiplicity, int).\n",
    "s=2     # The size of lower-level representations (int).\n",
    "L=2     # The number of levels in the hierarchy (int).\n",
    "\n",
    "input_size = s**L # number of pixels, actual input size is (input_size x num_features) because of one-hot encoding\n",
    "num_data = n * (m**((s**L-1)//(s-1))) # total number of data\n",
    "print(input_size, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE RULES AND DATA FROM LABELS\n",
    "\n",
    "# rules = sample_rules( v, v, m, s, L, seed=42)\n",
    "# for l in range(L):\n",
    "#     print(f'level {l}, rules:')\n",
    "#     for i in range(v):\n",
    "#         print(f'{i}->{list(rules[l][i])}')\n",
    "\n",
    "# labels = torch.randint(low=0, high=n, size=(32,))\n",
    "# features, labels = sample_data_from_labels_unif(labels, rules)\n",
    "# for i in range(features.size(0)):\n",
    "#     print(features[i,:], labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMPLING WITH REPLACEMENT (REQUIRED FOR DATASET LARGER THAN sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling seed: 99999999\n"
     ]
    }
   ],
   "source": [
    "random.seed()\n",
    "seed_rules = 12345678   # seed of the random hierarchy model\n",
    "seed_sample = 99999999  # random.randrange(10000000,99999999)\n",
    "print('sampling seed:', seed_sample)\n",
    "\n",
    "train_size = 4 # size of the training set\n",
    "test_size = 0 # size of the test set\n",
    "input_format = 'onehot' # alternative: onehot\n",
    "# to generate the full dataset: set trainset=num_data, test_size=0\n",
    "bonus = {}\n",
    "\n",
    "dataset = datasets.RandomHierarchyModel(\n",
    "    num_features=v, # vocabulary size\n",
    "    num_synonyms=m, # features multiplicity\n",
    "    num_layers=L,   # number of layers\n",
    "    num_classes=n,  # number of classes\n",
    "    tuple_size=s,   # number of branches of the tree\n",
    "    seed_rules=seed_rules,\n",
    "    seed_sample=seed_sample,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    input_format=input_format,\n",
    "    whitening=0, # 1 to whiten the input\n",
    "    replacement=True,\n",
    "    bonus=bonus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0, rules:\n",
      "0->[tensor([0, 3]), tensor([2, 2])]\n",
      "1->[tensor([2, 1]), tensor([3, 3])]\n",
      "2->[tensor([1, 0]), tensor([3, 1])]\n",
      "3->[tensor([2, 0]), tensor([3, 2])]\n",
      "level 1, rules:\n",
      "0->[tensor([0, 2]), tensor([3, 0])]\n",
      "1->[tensor([1, 1]), tensor([0, 1])]\n",
      "2->[tensor([3, 3]), tensor([3, 1])]\n",
      "3->[tensor([0, 0]), tensor([1, 3])]\n",
      "torch.Size([4, 4, 4])\n",
      "tensor(0.2500)\n",
      "tensor(1.)\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.]]) tensor(1)\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.]]) tensor(0)\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0.]]) tensor(0)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]]) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for l in range(L):\n",
    "    print(f'level {l}, rules:')\n",
    "    for i in range(v):\n",
    "        print(f'{i}->{list(dataset.rules[l][i])}')\n",
    "\n",
    "x = dataset.features\n",
    "print(x.size())\n",
    "\n",
    "if 'onehot' in input_format:\n",
    "    print(x.mean(dim=1).mean())\n",
    "    print(x.norm(dim=1).mean())\n",
    "    for i in range(x.size(0)):\n",
    "        print(x[i,:], dataset.labels[i])\n",
    "\n",
    "elif 'long' in input_format:\n",
    "    for i in range(x.size(0)):\n",
    "        print(x[i,:], dataset.labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PRODUCTION RULES DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 32\n",
      "sampling seed: 99999999\n"
     ]
    }
   ],
   "source": [
    "v=8    # The number of values each variable can take (vocabulary size, int).\n",
    "n=v    # The number of classes (int).\n",
    "m=v//2     # The number of synonymic lower-level representations (multiplicity, int).\n",
    "s=2     # The size of lower-level representations (int).\n",
    "L=1     # The number of levels in the hierarchy (int).\n",
    "\n",
    "input_size = s**L # number of pixels, actual input size is (input_size x num_features) because of one-hot encoding\n",
    "num_data = n * (m**((s**L-1)//(s-1))) # total number of data\n",
    "print(input_size, num_data)\n",
    "\n",
    "random.seed()\n",
    "seed_rules = 12345678   # seed of the random hierarchy model\n",
    "seed_sample = 99999999  # random.randrange(10000000,99999999)\n",
    "print('sampling seed:', seed_sample)\n",
    "\n",
    "probability = {}\n",
    "for l in range(L-1):\n",
    "    probability[l] = torch.ones(m)/m\n",
    "# lognormal = torch.randn(m).exp()\n",
    "# probability[L-1] = lognormal /lognormal.sum()\n",
    "rank1 = torch.zeros(m)\n",
    "rank1[0] = 1.0\n",
    "probability[L-1] = rank1\n",
    "\n",
    "train_size = 2**20 # size of the training set\n",
    "test_size = 0 # size of the test set\n",
    "input_format = 'onehot_tuples' # alternative: onehot\n",
    "# to generate the full dataset: set trainset=num_data, test_size=0\n",
    "\n",
    "dataset = datasets.RandomHierarchyModel(\n",
    "    num_features=v, # vocabulary size\n",
    "    num_synonyms=m, # features multiplicity\n",
    "    num_layers=L,   # number of layers\n",
    "    num_classes=n,  # number of classes\n",
    "    tuple_size=s,   # number of branches of the tree\n",
    "    probability=probability,\n",
    "    seed_rules=seed_rules,\n",
    "    seed_sample=seed_sample,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    input_format=input_format,\n",
    "    whitening=0, # 1 to whiten the input\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n",
      "tensor([12, 40, 37, 14])\n",
      "torch.Size([1048576, 64, 1])\n",
      "tensor([0.9996, 0.0000, 0.0000, 0.0000])\n",
      "tensor([1., 0., 0., 0.])\n",
      "1048576 tensor(3.6380e-08)\n"
     ]
    }
   ],
   "source": [
    "rep0 = dataset.rules[L-1][0]\n",
    "print(rep0.size())\n",
    "\n",
    "rep0_indices = base2dec(rep0, v)\n",
    "print(rep0_indices)\n",
    "\n",
    "x = dataset.features\n",
    "print(x.size())\n",
    "empirical = x.sum(dim=0).sum(dim=-1)/(s**(L-1))/train_size\n",
    "emp_extract = empirical[rep0_indices]*v\n",
    "print(emp_extract)\n",
    "print(dataset.probability[L-1])\n",
    "x = emp_extract-dataset.probability[L-1]\n",
    "print(train_size, x.var())\n",
    "\n",
    "# print(f'level {L-1}, rules:')\n",
    "# print(f'{0}->{list(dataset.rules[L-1][0])}, prob. {probability[L-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1048576, 64, 1])\n",
      "tensor([0.1250, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.1253, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "x = dataset.features\n",
    "print(x.size())\n",
    "empirical = x.sum(dim=0).sum(dim=-1)/(s**(L-1))/train_size\n",
    "empirical, _ = torch.sort(empirical, descending=True)\n",
    "empirical = empirical.reshape(v,-1)\n",
    "true, _ = torch.sort(dataset.probability[L-1], descending=True)\n",
    "print(true*1./v)\n",
    "print(empirical[:, 0])\n",
    "# print([0.8*5/16, 0.2*3/16, 0.8*3/16, 0., 0., 0.8*3/16, 0., 0.2*5/16, 0., 0., 0., 0., 0.2*3/16, 0.2*5/16, 0.8*5/16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARD SAMPLING (WITHOUT REPLACEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 32\n"
     ]
    }
   ],
   "source": [
    "v=4    # The number of values each variable can take (vocabulary size, int).\n",
    "n=v    # The number of classes (int).\n",
    "m=2     # The number of synonymic lower-level representations (multiplicity, int).\n",
    "s=2     # The size of lower-level representations (int).\n",
    "L=2     # The number of levels in the hierarchy (int).\n",
    "\n",
    "input_size = s**L # number of pixels, actual input size is (input_size x num_features) because of one-hot encoding\n",
    "num_data = n * (m**((s**L-1)//(s-1))) # total number of data\n",
    "print(input_size, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling seed: 94944558\n",
      "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_is_protocol', 'features', 'labels', 'num_classes', 'num_features', 'num_layers', 'num_synonyms', 'rules', 'transform', 'tuple_size']\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "random.seed()\n",
    "seed_rules = 12345678 # seed of the random hierarchy model\n",
    "seed_sample = random.randrange(10000000,99999999)\n",
    "print('sampling seed:', seed_sample)\n",
    "\n",
    "train_size = -1 # size of the training set\n",
    "test_size = 0 # size of the test set\n",
    "input_format = 'long' # alternative: onehot\n",
    "# to generate the full dataset: set trainset=num_data, test_size=0\n",
    "bonus = dict.fromkeys(['tree', 'noise', 'synonyms', 'size'])\n",
    "bonus['size'] = 4\n",
    "\n",
    "dataset = datasets.RandomHierarchyModel(\n",
    "    num_features=v, # vocabulary size\n",
    "    num_synonyms=m, # features multiplicity\n",
    "    num_layers=L, # number of layers\n",
    "    num_classes=n, # number of classes\n",
    "    tuple_size=s, # number of branches of the tree\n",
    "    seed_rules=seed_rules,\n",
    "    seed_sample=seed_sample,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    input_format=input_format,\n",
    "    whitening=0, # 1 to whiten the input\n",
    "    replacement=False,\n",
    "    bonus=bonus\n",
    ")\n",
    "\n",
    "print(dir(dataset)) \n",
    "# for the input points call trainset.input\n",
    "print(dataset.features.size()) # dimension: train_size x num_features x input_size\n",
    "# for the labels call trainset.output\n",
    "print(dataset.labels.size()) # dimension: train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 1, 1],\n",
      "        [1, 3, 2, 4],\n",
      "        [4, 1, 1, 1],\n",
      "        [4, 1, 2, 4],\n",
      "        [4, 4, 4, 4],\n",
      "        [4, 4, 4, 2],\n",
      "        [4, 2, 4, 4],\n",
      "        [4, 2, 4, 2],\n",
      "        [4, 4, 2, 2],\n",
      "        [4, 4, 1, 2],\n",
      "        [4, 2, 2, 2],\n",
      "        [4, 2, 1, 2],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 2, 4],\n",
      "        [2, 4, 1, 1],\n",
      "        [2, 4, 2, 4],\n",
      "        [2, 2, 1, 3],\n",
      "        [2, 2, 4, 1],\n",
      "        [1, 2, 1, 3],\n",
      "        [1, 2, 4, 1],\n",
      "        [1, 1, 2, 2],\n",
      "        [1, 1, 1, 2],\n",
      "        [2, 4, 2, 2],\n",
      "        [2, 4, 1, 2],\n",
      "        [4, 4, 1, 3],\n",
      "        [4, 4, 4, 1],\n",
      "        [4, 2, 1, 3],\n",
      "        [4, 2, 4, 1],\n",
      "        [1, 1, 4, 4],\n",
      "        [1, 1, 4, 2],\n",
      "        [2, 4, 4, 4],\n",
      "        [2, 4, 4, 2]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([3, 3, 3, 3])\n",
      "1 tensor([[3, 2],\n",
      "        [3, 2],\n",
      "        [3, 2],\n",
      "        [3, 2]])\n"
     ]
    }
   ],
   "source": [
    "for k in bonus['tree'].keys():\n",
    "    print(k,bonus['tree'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([[4, 4, 1, 3],\n",
      "        [1, 1, 4, 2],\n",
      "        [4, 2, 1, 3],\n",
      "        [4, 2, 4, 1]])\n",
      "1 tensor([[2, 4, 4, 4],\n",
      "        [1, 1, 4, 2],\n",
      "        [1, 1, 4, 4],\n",
      "        [1, 1, 4, 2]])\n"
     ]
    }
   ],
   "source": [
    "for k in bonus['synonyms'].keys():\n",
    "    print(k,bonus['synonyms'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 4, 2],\n",
      "        [4, 2, 4, 4],\n",
      "        [2, 4, 2, 4]])\n",
      "1 tensor([[1, 3, 4, 4],\n",
      "        [1, 1, 4, 2],\n",
      "        [4, 1, 4, 4],\n",
      "        [1, 2, 4, 2]])\n",
      "0 tensor([[1, 1, 3, 4],\n",
      "        [1, 1, 4, 2],\n",
      "        [2, 4, 1, 4],\n",
      "        [2, 4, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "for k in bonus['noise'].keys():\n",
    "    print(k,bonus['noise'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n",
      "tensor([1, 3, 1, 1]) tensor(0)\n",
      "tensor([1, 3, 2, 4]) tensor(0)\n",
      "tensor([4, 1, 1, 1]) tensor(0)\n",
      "tensor([4, 1, 2, 4]) tensor(0)\n",
      "tensor([4, 4, 4, 4]) tensor(0)\n",
      "tensor([4, 4, 4, 2]) tensor(0)\n",
      "tensor([4, 2, 4, 4]) tensor(0)\n",
      "tensor([4, 2, 4, 2]) tensor(0)\n",
      "tensor([4, 4, 2, 2]) tensor(1)\n",
      "tensor([4, 4, 1, 2]) tensor(1)\n",
      "tensor([4, 2, 2, 2]) tensor(1)\n",
      "tensor([4, 2, 1, 2]) tensor(1)\n",
      "tensor([1, 1, 1, 1]) tensor(1)\n",
      "tensor([1, 1, 2, 4]) tensor(1)\n",
      "tensor([2, 4, 1, 1]) tensor(1)\n",
      "tensor([2, 4, 2, 4]) tensor(1)\n",
      "tensor([2, 2, 1, 3]) tensor(2)\n",
      "tensor([2, 2, 4, 1]) tensor(2)\n",
      "tensor([1, 2, 1, 3]) tensor(2)\n",
      "tensor([1, 2, 4, 1]) tensor(2)\n",
      "tensor([1, 1, 2, 2]) tensor(2)\n",
      "tensor([1, 1, 1, 2]) tensor(2)\n",
      "tensor([2, 4, 2, 2]) tensor(2)\n",
      "tensor([2, 4, 1, 2]) tensor(2)\n",
      "tensor([4, 4, 1, 3]) tensor(3)\n",
      "tensor([4, 4, 4, 1]) tensor(3)\n",
      "tensor([4, 2, 1, 3]) tensor(3)\n",
      "tensor([4, 2, 4, 1]) tensor(3)\n",
      "tensor([1, 1, 4, 4]) tensor(3)\n",
      "tensor([1, 1, 4, 2]) tensor(3)\n",
      "tensor([2, 4, 4, 4]) tensor(3)\n",
      "tensor([2, 4, 4, 2]) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "x = dataset.features\n",
    "print(x.size())\n",
    "\n",
    "if 'onehot' in input_format:\n",
    "    print(x.mean(dim=1).mean())\n",
    "    print(x.norm(dim=1).mean())\n",
    "\n",
    "elif 'long' in input_format:\n",
    "    for i in range(x.size(0)):\n",
    "        print(x[i,:], dataset.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rules: list of length  2 ,\n",
      "first element of size  torch.Size([4, 2, 2]) , (num_classes x num_synonyms x tuple_size)\n",
      "2-th element of size  torch.Size([4, 2, 2]) , (num_features x num_synonyms x tuple_size)\n",
      "rules[l][v,j] = j-th rep of the v-th level-(L-l) feature,\n",
      "e.g. list of tuples corresponding to class 0:\n",
      "tensor([[0, 3],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "L = len(dataset.rules)\n",
    "print('rules: list of length ', len(dataset.rules), ',')\n",
    "print('first element of size ', dataset.rules[0].size(), ', (num_classes x num_synonyms x tuple_size)')\n",
    "\n",
    "for l in range(1,L):\n",
    "    print(f'{l+1}-th element of size ', dataset.rules[l].size(), ', (num_features x num_synonyms x tuple_size)')\n",
    "print('rules[l][v,j] = j-th rep of the v-th level-(L-l) feature,')\n",
    "print('e.g. list of tuples corresponding to class 0:')\n",
    "print(dataset.rules[0][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PERTURBATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_from_indices(samples, rules, v, n, m, s, L, bonus):\n",
    "    \"\"\"\n",
    "    Create data of the Random Hierarchy Model starting from a set of rules and the sampled indices.\n",
    "\n",
    "    Args:\n",
    "        samples: A tensor of size [batch_size, I], with I from 0 to max_data-1, containing the indices of the data to be sampled.\n",
    "        rules: A dictionary containing the rules for each level of the hierarchy.\n",
    "        n: The number of classes (int).\n",
    "        m: The number of synonymic lower-level representations (multiplicity, int).\n",
    "        s: The size of lower-level representations (int).\n",
    "        L: The number of levels in the hierarchy (int).\n",
    "        bonus: Dictionary for additional output (list), includes 'noise' (randomly replace one symbol at each level), . Includes 'size' for the number of additional data. TODO: add custom positions for 'noise'\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the inputs and outputs of the model (plus additional output in bonus dict).\n",
    "    \"\"\"\n",
    "    max_data = n * m ** ((s**L-1)//(s-1))\n",
    "    data_per_hl = max_data // n \t# div by num_classes to get number of data per class\n",
    "\n",
    "    high_level = samples.div(data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get class index (run in range(n))\n",
    "    low_level = samples % data_per_hl\t\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "    labels = high_level\t\t# labels are the classes (features of highest level)\n",
    "    features = labels\t\t# init input features as labels (rep. size 1)\n",
    "    size = 1\n",
    "\n",
    "    if bonus:\n",
    "        if 'size' not in bonus.keys():\n",
    "            bonus['size'] = samples.size(0)\n",
    "        if 'noise' in bonus:\t# add corrupted version of the last bonus[-1] data\n",
    "            noise = {}\n",
    "            noise[L] = torch.clone(features[-bonus['size']:])\t# copy current representation (labels)...\n",
    "            noise[L][:] = torch.randint(n, (bonus['size'],))\t# ...and randomly change it\n",
    "            bonus['noise'] = noise\n",
    "        if 'synonyms' in bonus:\n",
    "            synonyms = {}\n",
    "            bonus['synonyms'] = synonyms\n",
    "\n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        choices = m**(size)\n",
    "        data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level feature\n",
    "\n",
    "        high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "        high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "        if 'synonyms' in bonus:\n",
    "\n",
    "            for ell in synonyms.keys():\t# propagate modified data down the tree\n",
    "                synonyms[ell] = rules[l][synonyms[ell], high_level[-bonus['size']:]]\n",
    "                synonyms[ell] = synonyms[ell].flatten(start_dim=1)\n",
    "\n",
    "            high_level_syn = torch.clone(high_level[-bonus['size']:]) # copy current representation indices...\n",
    "            if l==0:\n",
    "                high_level_syn[:] = torch.randint(m, (high_level_syn.size(0),)) # ... and randomly change it (only one index at the highest level)\n",
    "            else:\n",
    "                high_level_syn[:,-2] = torch.randint(m, (high_level_syn.size(0),))# ... and randomly change the next-to-last\n",
    "            synonyms[L-l] = torch.clone(features[-bonus['size']:])\n",
    "            synonyms[L-l] = rules[l][synonyms[L-l], high_level_syn]\n",
    "            synonyms[L-l] = synonyms[L-l].flatten(start_dim=1)\n",
    "            #TODO: add custom positions for 'synonyms'\n",
    "        \n",
    "        features = rules[l][features, high_level]\t        \t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "        features = features.flatten(start_dim=1)\t\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "        size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "        low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "        if 'noise' in bonus:\n",
    "\n",
    "            for ell in noise.keys():\t# propagate modified data down the tree\n",
    "                noise[ell] = rules[l][noise[ell], high_level[-bonus['size']:]]\n",
    "                noise[ell] = noise[ell].flatten(start_dim=1)\n",
    "\n",
    "            noise[L-l-1] = torch.clone(features[-bonus['size']:])\t# copy current representation ...\n",
    "            noise[L-l-1][:,-2] = torch.randint(v, (bonus['size'],))\t# ... and randomly change the next-to-last feature\n",
    "            #TODO: add custom positions for 'noise'\n",
    "\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "v = 32\n",
    "m = 2\n",
    "\n",
    "L = 2\n",
    "s = 2\n",
    "\n",
    "input_size = s**L # number of pixels, actual input size is (input_size x num_features) because of one-hot encoding\n",
    "max_data = n * (m**((s**L-1)//(s-1))) # total number of data\n",
    "print(input_size, max_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLE NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_rules = 12345678 # seed of the random hierarchy model\n",
    "rules = sample_rules( v, n, m, s, L, seed=seed_rules)\n",
    "samples = torch.arange( max_data)\n",
    "\n",
    "bonus = dict.fromkeys(['noise', 'size'])\n",
    "bonus['size'] = 4\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = n * m ** ((s**L-1)//(s-1))\n",
    "data_per_hl = max_data // n \t# div by num_classes to get number of data per class\n",
    "\n",
    "high_level = samples.div(data_per_hl, rounding_mode='floor')\n",
    "low_level = samples % data_per_hl\t\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "labels = high_level\t\t# labels are the classes (features of highest level)\n",
    "features = labels\t\t# init input features as labels (rep. size 1)\n",
    "size = 1\n",
    "\n",
    "if 'noise' in bonus:\n",
    "    noise = {}\n",
    "    noise[L] = torch.clone(features[-bonus['size']:]) # copy current representation (labels)...\n",
    "    noise[L][:] = torch.randint(n, (bonus['size'],)) # ...and randomly change it\n",
    "    bonus['noise'] = noise\n",
    "\n",
    "print(features)\n",
    "print(noise[L])\n",
    "\n",
    "\n",
    "for l in range(L):\n",
    "\n",
    "    choices = m**size\n",
    "    data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level featur\n",
    "\n",
    "    high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "    high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "    features = rules[l][features, high_level]\t\t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "    features = features.flatten(start_dim=1)\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "    size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "    low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "    if 'noise' in bonus:\n",
    "        for ell in noise.keys(): # propagate modified data\n",
    "            noise[ell] = rules[l][noise[ell], high_level[-bonus['size']:]]\n",
    "            noise[ell] = noise[ell].flatten(start_dim=1)\n",
    "\n",
    "        noise[L-l-1] = torch.clone(features[-bonus['size']:]) # copy current representation ...\n",
    "        noise[L-l-1][:,-2] = torch.randint(v, (bonus['size'],)) # ... and randomly change the next-to-last feature\n",
    "\n",
    "    print(features)\n",
    "\n",
    "    for key in noise.keys():\n",
    "        print(noise[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bonus['noise'])\n",
    "print(bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testable by looking at the rule matrices rules[0] and rules[1] (recall rules[l][v,j] = j-th rep of the v-th level-(L-l) feature) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SAMPLE SYNONYMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_rules = 12345678 # seed of the random hierarchy model\n",
    "rules = sample_rules( v, n, m, s, L, seed=seed_rules)\n",
    "samples = torch.arange( max_data)\n",
    "\n",
    "bonus = dict.fromkeys(['synonyms', 'size'])\n",
    "bonus['size'] = 4\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = n * m ** ((s**L-1)//(s-1))\n",
    "data_per_hl = max_data // n \t# div by num_classes to get number of data per class\n",
    "\n",
    "high_level = samples.div(data_per_hl, rounding_mode='floor')\n",
    "low_level = samples % data_per_hl\t\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "labels = high_level\t\t# labels are the classes (features of highest level)\n",
    "features = labels\t\t# init input features as labels (rep. size 1)\n",
    "size = 1\n",
    "\n",
    "if 'synonyms' in bonus:\n",
    "    synonyms = {}\n",
    "    bonus['synonyms'] = synonyms\n",
    "\n",
    "print(features)\n",
    "\n",
    "for l in range(L):\n",
    "\n",
    "    choices = m**size\n",
    "    data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level featur\n",
    "\n",
    "    high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "    high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "    if 'synonyms' in bonus:\n",
    "\n",
    "        for ell in synonyms.keys(): # propagate modified data\n",
    "            synonyms[ell] = rules[l][synonyms[ell], high_level[-bonus['size']:]]\n",
    "            synonyms[ell] = synonyms[ell].flatten(start_dim=1)\n",
    "\n",
    "        high_level_syn = torch.clone(high_level[-bonus['size']:])\n",
    "        if l==0:\n",
    "            high_level_syn[:] = torch.randint(m, (high_level_syn.size(0),))\n",
    "        else:\n",
    "            high_level_syn[:,-2] = torch.randint(m, (high_level_syn.size(0),))\n",
    "        synonyms[L-l] = torch.clone(features[-bonus['size']:])\n",
    "        synonyms[L-l] = rules[l][synonyms[L-l], high_level_syn]\n",
    "        synonyms[L-l] = synonyms[L-l].flatten(start_dim=1)\n",
    "        print(high_level_syn)\n",
    "        for key in synonyms.keys():\n",
    "            print(synonyms[key])\n",
    "\n",
    "    features = rules[l][features, high_level]\t\t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "    features = features.flatten(start_dim=1)\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "    size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "    low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "    print(high_level)\n",
    "    print(features)\n",
    "\n",
    "# for l in range(L):\n",
    "\n",
    "#     choices = m**size\n",
    "#     data_per_hl = data_per_hl // choices\t# div by num_choices to get number of data per high-level featur\n",
    "\n",
    "#     high_level = low_level.div( data_per_hl, rounding_mode='floor')\t# div by data_per_hl to get high-level feature index (1 index in range(m**size))\n",
    "#     high_level = dec2base(high_level, m, length=size).squeeze()\t# convert to base m (size indices in range(m), squeeze needed if index already in base m)\n",
    "\n",
    "#     features = rules[l][features, high_level]\t\t\t# apply l-th rule to expand to get features at the lower level (tensor of size (batch_size, size, s))\n",
    "#     features = features.flatten(start_dim=1)\t\t\t# flatten to tensor of size (batch_size, size*s)\n",
    "#     size *= s\t\t\t\t\t\t\t\t# rep. size increases by s at each level\n",
    "#     low_level = low_level % data_per_hl\t\t\t\t# compute remainder (run in range(data_per_hl))\n",
    "\n",
    "#     if 'noise' in bonus:\n",
    "#         for ell in noise.keys(): # propagate modified data\n",
    "#             noise[ell] = rules[l][noise[ell], high_level[-bonus['size']:]]\n",
    "#             noise[ell] = noise[ell].flatten(start_dim=1)\n",
    "\n",
    "#         noise[L-l-1] = torch.clone(features[-bonus['size']:]) # copy current representation ...\n",
    "#         noise[L-l-1][:,-2] = torch.randint(v, (bonus['size'],)) # ... and randomly change the next-to-last feature\n",
    "\n",
    "#     print(features)\n",
    "\n",
    "#     for key in noise.keys():\n",
    "#         print(noise[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rhm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
